# A1

# 目录

1. 功能概述
2. 整体结构
3. Python 端实现
    - 3.1 WebSocket 服务器
    - 3.2 人脸识别与情绪推断代码
    - 3.3 情绪数据发送逻辑
4. Unreal Engine 端实现
    - 4.1 UMyGameInstance 与 WebSocket 连接
    - 4.2 接收事件与 Blueprint 协作
    - 4.3 角色（AEmotionActor）的 Timelines / RPC
5. 总结

---

# 1.系统概述

本系统旨在通过摄像头实时分析玩家的表情并估计情绪，将结果应用于 Unreal Engine 中角色的表情动画。

Python 端使用 OpenCV 和 MTCNN 进行面部检测，并使用预训练的 Keras 模型（fer2013_mini_XCEPTION）进行情绪分类。将识别结果（如快乐、悲伤、惊讶等）通过 WebSocket 定期发送。UE 端根据接收到的字符串使用 Timeline 进行形态目标动画。

# 2.整体结构

1. **Python**
    - 捕获摄像头视频
    - 进行人脸识别和情绪推断
    - 每5秒通过 WebSocket 服务器发送
2. **WebSocket 服务器**
    - 在 `localhost:8765` 监听
    - 将接收的消息广播给所有客户端
3. **Unreal Engine**
    - `UMyGameInstance` 接收到 `OnEmotionReceivedA/B`
    - 触发 Blueprint 事件，`AEmotionActor` 调用 RPC
    - 通过 Timeline 控制形态目标变形

```
bash
复制
PlayerA / PlayerB
├── Python (情绪推断 + WebSocket 服务器)
│   ├── OpenCV + MTCNN → 人脸检测
│   ├── Keras + fer2013_mini_XCEPTION → 情绪分类
│   ├── WebSocket 服务器 (websockets.serve)
│   ├── 将情绪数据发送至 /localhost:8765
│
└── Unreal Engine (GameInstance + Actor)
    ├── UMyGameInstance 中初始化 WebSocketA / WebSocketB
    ├── OnMessage 通过 Blueprint 的 OnEmotionReceivedA/B 处理
    ├── AEmotionActor 利用 RPC (Server/Multicast) 控制 Timelines
    └── 形态目标变形

```

---

# 3.Python 端实现

## 3.1 WebSocket 服务器 (websockets.serve)

```
python
复制
import asyncio
import websockets

connected_clients = set()

async def handler(websocket, path):
    connected_clients.add(websocket)
    try:
        async for message in websocket:
            for client in connected_clients:
                if client != websocket:
                    await client.send(message)
    finally:
        connected_clients.remove(websocket)

async def main():
    async with websockets.serve(handler, "localhost", 8765):
        print("WebSocket 服务器已启动在 ws://localhost:8765")
        await asyncio.Future()  # 永久等待

```

## 3.2 人脸识别与情绪推断代码

```
python
复制
import cv2
from mtcnn import MTCNN
from keras.models import load_model

detector = MTCNN()
emotion_model = load_model("fer2013_mini_XCEPTION.h5", compile=False)
video_capture = cv2.VideoCapture(0)

while True:
    ret, frame = video_capture.read()
    faces = detector.detect_faces(frame)
    # 截取面部区域并进行情绪推断

```

## 3.3 情绪数据发送逻辑

```
scss
复制
import datetime
import asyncio

def send_emotion_data(emotion):
    asyncio.run(websockets.connect("ws://localhost:8765").send(emotion))

if (datetime.datetime.now() - last_sent_time).total_seconds() >= 5:
    send_emotion_data(most_common_emotion)

```

---

# 4.Unreal Engine 端实现

## 4.1 UMyGameInstance 与 WebSocket 连接

```
rust
复制
void UMyGameInstance::Init()
{
    if (!FModuleManager::Get().IsModuleLoaded("WebSockets"))
    {
        FModuleManager::Get().LoadModule("WebSockets");
    }

    WebSocketA = FWebSocketsModule::Get().CreateWebSocket("ws://localhost:8765");
    WebSocketA->OnMessage().AddUObject(this, &UMyGameInstance::OnEmotionReceivedA);
    WebSocketA->Connect();
}

```

## 4.2 接收事件与 Blueprint 协作

```
cpp
复制
UFUNCTION(BlueprintImplementableEvent, Category = "WebSockets")
void OnEmotionReceivedA(const FString& Emotion);

```

## 4.3 角色（AEmotionActor）的 Timelines / RPC

```
php
复制
void AEmotionActor::Server_TriggerHappy_Implementation()
{
    if (CurrentEmotion != EEmotionState::None) return;
    CurrentEmotion = EEmotionState::Happy;
    Multicast_StartHappy();
}

```

---

# 5.总结

1. **Python** 端处理摄像头视频 → 人脸识别 → 情绪推断 → WebSocket 发送
2. **WebSocket 服务器** 对所有客户端进行广播
3. **UE** 通过 `UMyGameInstance` 接收 → 通过 `Blueprint` 处理 → `AEmotionActor` 调用 RPC 执行动画
4. 通过 `Timeline` 控制形态目标，实现表情变化