# Emotion-Driven Animation Modules

# 

## 目次

1. システム概要
2. システム構成
3. Python 側の実装
    
    - 3.1 WebSocket サーバー
    
    - 3.2 顔認識と感情推定コード
    
    - 3.3 感情データ送信ロジック
    
4. Unreal Engine 側の実装
    
    - 4.1 UMyGameInstance による WebSocket 接続
    
    - 4.2 イベント受信と Blueprint 協調
    
    - 4.3 キャラクター（AEmotionActor）の Timeline / RPC 処理
    
5. まとめ

---

## 1. システム概要

本システムは、カメラを通じてプレイヤーの表情をリアルタイムで分析し、推定された感情を Unreal Engine 内のキャラクターのアニメーションに反映させることを目的としています。

Python 側では OpenCV と MTCNN を用いて顔を検出し、事前学習済みの Keras モデル（fer2013_mini_XCEPTION）によって感情（喜び、悲しみ、驚きなど）を分類します。推定された感情結果は一定の間隔（5秒）で WebSocket 経由で Unreal Engine に送信されます。UE 側では、受信された感情データに基づいて Timeline によりモーフターゲットを制御し、キャラクターの表情アニメーションを実現します。

---

## 2. システム構成

以下はシステムの簡易的な全体フローです：
![全体構成図](/img/DosImage/A1.png)

### 1. **Python**

- カメラから映像をキャプチャ
- 顔検出と感情分類を実行
- 5秒ごとに WebSocket サーバーへ送信

### 2. **WebSocket サーバー**

- `localhost:8765` で待機
- 受信した感情データを全クライアントにブロードキャスト

### 3. **Unreal Engine**

- `UMyGameInstance` にて WebSocket 接続
- 受信イベント `OnEmotionReceivedA/B` を Blueprint へ転送
- `AEmotionActor` にて RPC を用いて Timeline 実行
- 形状変形アニメーションで感情表現を再現



---

## 3. Python 側の実装

### 3.1 WebSocket サーバーの構築（`websockets.serve` 使用）

```python
python
复制编辑
import asyncio
import websockets

connected_clients = set()

async def handler(websocket, path):
    connected_clients.add(websocket)
    try:
        async for message in websocket:
            for client in connected_clients:
                if client != websocket:
                    await client.send(message)
    finally:
        connected_clients.remove(websocket)

async def main():
    async with websockets.serve(handler, "localhost", 8765):
        print("WebSocket サーバーが ws://localhost:8765 で起動しました")
        await asyncio.Future()  # 永遠に待機

```

---

### 3.2 顔認識と感情推定処理

```python
python
复制编辑
import cv2
from mtcnn import MTCNN
from keras.models import load_model

detector = MTCNN()
emotion_model = load_model("fer2013_mini_XCEPTION.h5", compile=False)
video_capture = cv2.VideoCapture(0)

while True:
    ret, frame = video_capture.read()
    faces = detector.detect_faces(frame)
    # 顔領域を抽出し、感情を推定

```

---

### 3.3 感情データの送信ロジック

```python
python
复制编辑
import datetime
import asyncio

def send_emotion_data(emotion):
    asyncio.run(websockets.connect("ws://localhost:8765").send(emotion))

if (datetime.datetime.now() - last_sent_time).total_seconds() >= 5:
    send_emotion_data(most_common_emotion)

```

---

## 4. Unreal Engine 側の実装

### 4.1 UMyGameInstance による WebSocket 接続

```cpp
cpp
复制编辑
void UMyGameInstance::Init()
{
    if (!FModuleManager::Get().IsModuleLoaded("WebSockets"))
    {
        FModuleManager::Get().LoadModule("WebSockets");
    }

    WebSocketA = FWebSocketsModule::Get().CreateWebSocket("ws://localhost:8765");
    WebSocketA->OnMessage().AddUObject(this, &UMyGameInstance::OnEmotionReceivedA);
    WebSocketA->Connect();
}

```

---

### 4.2 Blueprint 連携イベント

```cpp
cpp
复制编辑
UFUNCTION(BlueprintImplementableEvent, Category = "WebSockets")
void OnEmotionReceivedA(const FString& Emotion);

```

---

### 4.3 キャラクター（AEmotionActor）の Timeline / RPC 処理

```cpp
cpp
复制编辑
void AEmotionActor::Server_TriggerHappy_Implementation()
{
    if (CurrentEmotion != EEmotionState::None) return;
    CurrentEmotion = EEmotionState::Happy;
    Multicast_StartHappy();
}

```

---

## 5. まとめ

- Python 側でカメラ映像を処理し、顔検出と感情推定を行い、WebSocket 経由でデータを送信
- WebSocket サーバーは全クライアントへデータをブロードキャスト
- Unreal Engine 側では `UMyGameInstance` が感情文字列を受信し、Blueprint を通じて `AEmotionActor` に伝達
- RPC と Timeline により、キャラクターのモーフターゲットが制御され、リアルタイムな表情アニメーションが実現される

#